{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从S3读取文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark读S3关键配置：    \n",
    "    https://sparkour.urizone.net/recipes/using-s3/      \n",
    "    https://www.jitsejan.com/using-spark-to-read-from-s3.html     \n",
    "    https://gist.github.com/claudinei-daitx/3766d01b070f3f0f8d1b64fd06b71585 \n",
    "    \n",
    "`$SPARK_HOME/jars`下要包含hadoop-aws相关jar包，以目前版本为例：aws-java-sdk-1.7.4.jar、hadoop-aws-2.7.7.jar   \n",
    "一般上述jar包在 `$HADOOP_HOME/share/hadoop/tools/lib/` 下均可找到，复制到`$SPARK_HOME/jars` 下即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"dl.cfg\")\n",
    "\n",
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可直接指定包配置来启动，运行时会自动下载，这样 sparkSession 初始化会比较缓慢，不推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.7\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从S3读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取单个文件\n",
    "df_song = spark.read.format('json').load('s3a://udacity-dend/song_data/A/A/A/TRAAAAK128F9318786.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist_id: string, artist_latitude: double, artist_location: string, artist_longitude: double, artist_name: string, duration: double, num_songs: bigint, song_id: string, title: string, year: bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取多个文件\n",
    "df_songs = spark.read.format('json').load('s3a://udacity-dend/song_data/A/A/A/*.json')\n",
    "# cache到本地，下次读取加快速度\n",
    "df_songs.cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_songs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist_id='ARTC1LV1187B9A4858'),\n",
       " Row(artist_id='ARA23XO1187B9AF18F'),\n",
       " Row(artist_id='ARSVTNL1187B992A91'),\n",
       " Row(artist_id='AR73AIO1187B9AD57B'),\n",
       " Row(artist_id='ARXQBR11187B98A2CC')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_songs.select('artist_id').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若文件巨大，cache慎用\n",
    "df = spark.read.csv(\"s3a://udacity-dend/pagila/payment/payment.csv\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|payment_id;custom...|\n",
      "|16050;269;2;7;1.9...|\n",
      "|16051;269;1;98;0....|\n",
      "|16052;269;2;678;6...|\n",
      "|16053;269;2;703;0...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16050"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现CSV默认以逗号分隔，但上述文件是分号，故读出来只有一列，且应指定表头"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指定分隔符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[payment_id: int, customer_id: int, staff_id: int, rental_id: int, amount: double, payment_date: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里换一种方法读 csv，本质是一样的\n",
    "df = spark.read\\\n",
    "    .format('csv')\\\n",
    "    .option('sep',';')\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .option('header',True)\\\n",
    "    .load(\"s3a://udacity-dend/pagila/payment/payment.csv\")\n",
    "\n",
    "# 若文件巨大，cache慎用\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 成功分列\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|        payment_date|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|     16050|        269|       2|        7|  1.99|2017-01-24 21:40:...|\n",
      "|     16051|        269|       1|       98|  0.99|2017-01-25 15:16:...|\n",
      "|     16052|        269|       2|      678|  6.99|2017-01-28 21:44:...|\n",
      "|     16053|        269|       2|      703|  0.99|2017-01-29 00:58:...|\n",
      "|     16054|        269|       1|      750|  4.99|2017-01-29 08:10:...|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据类型转换\n",
    "将 payment_date 列由字符串转换为日期类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      "\n",
      "+----------+-----------+--------+---------+------+-------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|       payment_date|\n",
      "+----------+-----------+--------+---------+------+-------------------+\n",
      "|     16050|        269|       2|        7|  1.99|2017-01-24 21:40:19|\n",
      "|     16051|        269|       1|       98|  0.99|2017-01-25 15:16:50|\n",
      "|     16052|        269|       2|      678|  6.99|2017-01-28 21:44:14|\n",
      "|     16053|        269|       2|      703|  0.99|2017-01-29 00:58:02|\n",
      "|     16054|        269|       1|      750|  4.99|2017-01-29 08:10:06|\n",
      "+----------+-----------+--------+---------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "df_new = df.withColumn('payment_date',fn.to_timestamp('payment_date','yyyy-MM-dd HH:mm:ss'))\n",
    "df_new.printSchema()\n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增加月份列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+-------------------+-----+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|       payment_date|month|\n",
      "+----------+-----------+--------+---------+------+-------------------+-----+\n",
      "|     16050|        269|       2|        7|  1.99|2017-01-24 21:40:19|    1|\n",
      "|     16051|        269|       1|       98|  0.99|2017-01-25 15:16:50|    1|\n",
      "|     16052|        269|       2|      678|  6.99|2017-01-28 21:44:14|    1|\n",
      "|     16053|        269|       2|      703|  0.99|2017-01-29 00:58:02|    1|\n",
      "|     16054|        269|       1|      750|  4.99|2017-01-29 08:10:06|    1|\n",
      "+----------+-----------+--------+---------+------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API 方法\n",
    "df_new = df_new.withColumn('month',fn.month('payment_date'))\n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+-------------------+-----+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|       payment_date|month|\n",
      "+----------+-----------+--------+---------+------+-------------------+-----+\n",
      "|     16050|        269|       2|        7|  1.99|2017-01-24 21:40:19|    1|\n",
      "|     16051|        269|       1|       98|  0.99|2017-01-25 15:16:50|    1|\n",
      "|     16052|        269|       2|      678|  6.99|2017-01-28 21:44:14|    1|\n",
      "|     16053|        269|       2|      703|  0.99|2017-01-29 00:58:02|    1|\n",
      "|     16054|        269|       1|      750|  4.99|2017-01-29 08:10:06|    1|\n",
      "+----------+-----------+--------+---------+------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL API 方法\n",
    "df_new.drop('momth') # 先删除上述已有 month 列\n",
    "df_new.createOrReplaceTempView('vw_df_payment')\n",
    "df_new = spark.sql('''\n",
    "    select payment_id\n",
    "          ,customer_id\n",
    "          ,staff_id\n",
    "          ,rental_id\n",
    "          ,amount\n",
    "          ,payment_date\n",
    "          ,month(payment_date) as month\n",
    "    from vw_df_payment\n",
    "''')\n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL聚合计算-计算月度利润"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Month|           Revenue|\n",
      "+-----+------------------+\n",
      "|    1| 4824.429999999856|\n",
      "|    2| 9631.879999999608|\n",
      "|    3|23886.560000002115|\n",
      "|    4|28559.460000003943|\n",
      "|    5|  514.180000000001|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.createOrReplaceTempView('vw_df_payment')\n",
    "spark.sql('''\n",
    "    select Month,sum(amount) as Revenue\n",
    "    from vw_df_payment\n",
    "    group by month\n",
    "    order by month\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16049"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16049"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取CSV，指定schema\n",
    "infer schema比较费时而且难免会有与实际不符的情况，下面我们指定schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType,StructField,IntegerType,DoubleType,DateType,TimestampType\n",
    "\n",
    "# my_schema = StructType([\n",
    "#     StructField('payment_id' ,IntegerType()),\n",
    "#     StructField('customer_id',IntegerType()),\n",
    "#     StructField('staff_id',IntegerType()),\n",
    "#     StructField('rental_id',IntegerType()),\n",
    "#     StructField('amount',DoubleType()),\n",
    "#     StructField('payment_date',TimestampType())\n",
    "# ])\n",
    "\n",
    "\n",
    "# 除上述定义方法外，如下 DDL风格的 schema 定义亦可 (varchar 用 string，datetime 用 timestamp )\n",
    "# 注：含 timestamp 类型时，一定要指定 timestampFormat\n",
    "# yyyy-MM-dd HH:mm:ss，如果用 yyyy-MM-dd HH:mm:ss.SSSSSS\n",
    "# spark会将 2017-01-24 21:40:19.996577+00 的微秒读为 996577000，并加到分钟\n",
    "# 所以只能舍掉 SSSSSS 秒，或先读为字符串，再另外处理\n",
    "\n",
    "my_schema = '''\n",
    "    payment_id int,\n",
    "    customer_id int,\n",
    "    staff_id int,\n",
    "    rental_id int,\n",
    "    amount double,\n",
    "    payment_date timestamp\n",
    "'''\n",
    "\n",
    "df = spark.read\\\n",
    "    .format('csv')\\\n",
    "    .schema(my_schema)\\\n",
    "    .option('sep',';')\\\n",
    "    .option('header',True)\\\n",
    "    .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\")\\\n",
    "    .load(\"s3a://udacity-dend/pagila/payment/payment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      "\n",
      "+----------+-----------+--------+---------+------+-------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|       payment_date|\n",
      "+----------+-----------+--------+---------+------+-------------------+\n",
      "|     16050|        269|       2|        7|  1.99|2017-01-24 21:40:19|\n",
      "|     16051|        269|       1|       98|  0.99|2017-01-25 15:16:50|\n",
      "|     16052|        269|       2|      678|  6.99|2017-01-28 21:44:14|\n",
      "|     16053|        269|       2|      703|  0.99|2017-01-29 00:58:02|\n",
      "|     16054|        269|       1|      750|  4.99|2017-01-29 08:10:06|\n",
      "+----------+-----------+--------+---------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(payment_date=datetime.datetime(2017, 1, 24, 21, 40, 19)),\n",
       " Row(payment_date=datetime.datetime(2017, 1, 25, 15, 16, 50)),\n",
       " Row(payment_date=datetime.datetime(2017, 1, 28, 21, 44, 14)),\n",
       " Row(payment_date=datetime.datetime(2017, 1, 29, 0, 58, 2)),\n",
       " Row(payment_date=datetime.datetime(2017, 1, 29, 8, 10, 6))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('payment_date').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Month|           Revenue|\n",
      "+-----+------------------+\n",
      "|    1| 4824.429999999856|\n",
      "|    2| 9631.879999999608|\n",
      "|    3|23886.560000002115|\n",
      "|    4|28559.460000003943|\n",
      "|    5|  514.180000000001|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('vw_df_payment')\n",
    "spark.sql('''\n",
    "    select month(payment_date) as Month,sum(amount) as Revenue\n",
    "    from vw_df_payment\n",
    "    group by month(payment_date)\n",
    "    order by month\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写文件到S3\n",
    "直接写到S3应该是比较慢的，可以先保存在本地再上传到S3，或保存到AWS EMR的HDFS中，再传到S3，虽然麻烦，可能会快一些     \n",
    "参考：[Link1](https://gist.github.com/bachwehbi/49e0035bdcf3d420a181415e02a189b7)、[Link2](https://stackoverflow.com/questions/42822483/extremely-slow-s3-write-times-from-emr-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接写到S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "run time:121.28353834152222 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "\n",
    "df.write.format('json')\\\n",
    "    .mode('overwrite')\\\n",
    "    .save('s3a://bucket-vincent/payment.json')\n",
    "\n",
    "print(f'\\nrun time:{time.time() - s} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先写到本地，再copy到S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "run time:30.206376314163208 s\n"
     ]
    }
   ],
   "source": [
    "import subprocess as sub\n",
    "s = time.time()\n",
    "\n",
    "df.write.format('json')\\\n",
    "    .mode('overwrite')\\\n",
    "    .save('/home/ghost/workdata/payment.json')\n",
    "\n",
    "s3_copy_cmd = [\n",
    "    'aws','s3','cp',\n",
    "    '/home/ghost/workdata/payment.json',\n",
    "    's3://bucket-vincent/payment.json',\n",
    "    '--recursive','--exclude','*.crc'\n",
    "]\n",
    "\n",
    "sub.run(s3_copy_cmd)\n",
    "\n",
    "print(f'\\nrun time:{time.time() - s} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 试验 aws s3 cp 命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../workdata/s3-out-songs.csv/part-00000-a70bbb19-4b3b-4a48-b076-b61bd0b3ecf3-c000.csv to s3://bucket-vincent/s3-out-songs.csv/part-00000-a70bbb19-4b3b-4a48-b076-b61bd0b3ecf3-c000.csv\n",
      "upload: ../workdata/s3-out-songs.csv/_SUCCESS to s3://bucket-vincent/s3-out-songs.csv/_SUCCESS\n",
      "\n",
      "run time:5.440941333770752 s\n"
     ]
    }
   ],
   "source": [
    "# S3 copy 命令，复制本地文件到 S3，同名自动覆盖\n",
    "!aws s3 cp /home/ghost/workdata/s3-out-songs.csv s3://bucket-vincent/s3-out-songs.csv --recursive --exclude '*.crc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用python调用aws s3 cp命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "run time:2.599433660507202 s\n"
     ]
    }
   ],
   "source": [
    "import subprocess as sub\n",
    "s = time.time()\n",
    "\n",
    "s3_copy_cmd = [\n",
    "    'aws','s3','cp',\n",
    "    '/home/ghost/workdata/s3-out-songs.csv',\n",
    "    's3://bucket-vincent/s3-out-songs.csv',\n",
    "    '--recursive','--exclude','*.crc'\n",
    "]\n",
    "sub.run(s3_copy_cmd)\n",
    "\n",
    "print(f'\\nrun time:{time.time() - s} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subprocess 相关方法试验 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aws-cli/1.16.211 Python/3.6.7 Linux/5.0.0-23-generic botocore/1.12.201\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess as sub\n",
    "# sub.run('aws',shell=True,capture_output=True)\n",
    "output = sub.check_output('aws --version',shell=True)\n",
    "output.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='aws --version', returncode=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 单个 string 的 cmd,要加上 shell=True\n",
    "sub.run('aws --version',shell=True)\n",
    "\n",
    "# 传 list，shell=True 可省略\n",
    "sub.run(['aws','version'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取运行结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aws-cli/1.16.211 Python/3.6.7 Linux/5.0.0-23-generic botocore/1.12.201\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess as sub\n",
    "output = sub.check_output('aws --version',shell=True)\n",
    "output.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有用的命令行元素拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aws',\n",
       " 's3',\n",
       " 'cp',\n",
       " '/home/ghost/workdata/s3-out-songs.csv',\n",
       " 's3://bucket-vincent/s3-out-songs.csv',\n",
       " '--recursive',\n",
       " '--exclude',\n",
       " '*.crc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shlex, subprocess\n",
    "command_line = \"aws s3 cp /home/ghost/workdata/s3-out-songs.csv s3://bucket-vincent/s3-out-songs.csv --recursive --exclude '*.crc'\"\n",
    "shlex.split(command_line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
