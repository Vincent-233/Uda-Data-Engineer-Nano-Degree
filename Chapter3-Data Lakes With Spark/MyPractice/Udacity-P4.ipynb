{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,DoubleType,DateType,TimestampType\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config.get('AWS','AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_spark_session():\n",
    "#     spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "#         .getOrCreate()\n",
    "#     return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session_local_test():\n",
    "    spark = SparkSession.builder\\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "            .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session_local_test()\n",
    "input_data = '/home/ghost/workdata/P4/'\n",
    "output_data = '/home/ghost/workdata/Out-P4/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_song_data\n",
    "以下已执行过，不用再执行，可能比较费时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    song_data = os.path.join(input_data,'song_data/*/*/*/*.json')\n",
    "\n",
    "    # read song data file\n",
    "    df = spark.read.format('json').load(song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select('song_id','title','artist_id','year','duration')\n",
    "\n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write\\\n",
    "        .partitionBy('year','artist_id')\\\n",
    "        .parquet(os.path.join(output_data,'songs.parquet'),mode = 'overwrite')\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude')\\\n",
    "        .distinct()\\\n",
    "        .withColumnRenamed('artist_name','name')\\\n",
    "        .withColumnRenamed('artist_location','location')\\\n",
    "        .withColumnRenamed('artist_latitude','latitude')\\\n",
    "        .withColumnRenamed('artist_longitude','longitude')\n",
    "\n",
    "    # write artists table to parquet files\n",
    "    artists_table.write\\\n",
    "        .parquet(os.path.join(output_data,'artists.parquet'),mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_log_data\n",
    "以下已执行过，不用再执行，可能比较费时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    log_data = os.path.join(input_data,'log-data/*.json')\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.format('json').load(log_data)\n",
    "\n",
    "    # filter by actions for song plays\n",
    "    df = df.where(\"page = 'NextSong'\")\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select('userid','firstname','lastname','gender','level')\\\n",
    "        .distinct()\\\n",
    "        .withColumnRenamed('userid','user_id')\\\n",
    "        .withColumnRenamed('firstname','first_name')\\\n",
    "        .withColumnRenamed('lastname','last_name')\n",
    "\n",
    "    # write users table to parquet files\n",
    "    users_table.write.parquet(os.path.join(output_data,'users.parquet'),mode = 'overwrite')\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x:datetime.fromtimestamp(x/1000),TimestampType())\n",
    "    df = df.withColumn('start_time',get_timestamp(df['ts']))\n",
    "    df = df.withColumn('year',year(df['start_time']))\\\n",
    "           .withColumn('month',month(df['start_time']))\n",
    "\n",
    "\n",
    "    # create temp view for furture use\n",
    "    df.select('start_time')\\\n",
    "        .distinct()\\\n",
    "        .createOrReplaceTempView('vw_df_StartTime')\n",
    "\n",
    "    # extract columns to create time table\n",
    "    time_table = spark.sql(\"\"\"\n",
    "        select start_time\n",
    "              ,hour(start_time) as hour\n",
    "              ,dayofmonth(start_time) as day\n",
    "              ,weekofyear(start_time) as week\n",
    "              ,month(start_time) as month\n",
    "              ,year(start_time) as year\n",
    "              ,dayofweek(start_time) as weekday\n",
    "        from vw_df_StartTime\n",
    "    \"\"\")\n",
    "\n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write\\\n",
    "            .partitionBy('year','month')\\\n",
    "            .parquet(os.path.join(output_data,'time.parquet'),mode = 'overwrite')\n",
    "\n",
    "    # read in song and artists data to use for songplays table\n",
    "    song_df = spark.read.format('parquet')\\\n",
    "        .load(os.path.join(output_data,'songs.parquet'))\n",
    "\n",
    "    artist_df = spark.read.format('parquet')\\\n",
    "        .load(os.path.join(output_data,'artists.parquet'))\n",
    "\n",
    "    # extract columns from joined datasets to create songplays table \n",
    "    songplays_table = df.join(song_df,[df.song == song_df.title, df.length == song_df.duration])\\\n",
    "        .join(artist_df,[song_df.artist_id == artist_df.artist_id, df.artist == artist_df.name])\\\n",
    "        .select(df.start_time, df.userId, df.level, song_df.song_id,song_df.artist_id,\n",
    "                df.sessionId,df.location, df.userAgent, df.year, df.month)\n",
    "\n",
    "    # rename columns and add songplay_id\n",
    "    songplays_table = songplays_table.withColumnRenamed('userid','user_id')\\\n",
    "        .withColumnRenamed('sessionid','session_id')\\\n",
    "        .withColumnRenamed('useragent','user_agent')\\\n",
    "        .withColumn(\"songplay_id\",monotonically_increasing_id())\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write\\\n",
    "            .partitionBy('year','month')\\\n",
    "            .parquet(os.path.join(output_data,'songplays.parquet'),mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.select('ts').limit(5).show()\n",
    "# df.ts == df['ts']\n",
    "# # df.select('ts','start_time').take(1)\n",
    "# print(df.select('start_time').distinct().count())\n",
    "# print(df.count())\n",
    "# df.select('start_time').distinct().createOrReplaceTempView('vw_df_StartTime')\n",
    "# df = df.withColumn('start_time',get_timestamp(df['ts']))\n",
    "# df.selectExpr('get_timestamp(ts)') # 报错，需要注册到　SQL　UDF中\n",
    "\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "# joinExpr = [df.song == song_df.title, df.length == song_df.duration, df.artist == song_df\n",
    "\n",
    "# songplays_table = \n",
    "# time_table.alias('b')\n",
    "# print(songplays_table.count())\n",
    "# print(song_df.count())\n",
    "# print(artist_df.count())\n",
    "# songplays_table.take(3)\n",
    "df = df.withColumn('year',year(df['start_time']))\\\n",
    "       .withColumn('month',month(df['start_time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6820\n",
      "6820\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(df.where(\"page = 'NextSong'\").count())\n",
    "print(users_table.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-09-10 06:51:25\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "ts = int(\"1284101485\")\n",
    "\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write artists table to parquet files\n",
    "artists_table.write.parquet(os.path.join(output_data,'artists.parquet'),mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4488\n",
      "drwxrwxr-x 4 ghost ghost    4096 8月  10 23:44 Out-P4\n",
      "drwxrwxr-x 2 ghost ghost    4096 8月  10 20:57 out-patient.json\n",
      "drwxrwxr-x 4 ghost ghost    4096 8月  10 21:44 out-patient.parquet\n",
      "drwxrwxr-x 2 ghost ghost    4096 8月  10 20:56 out-song.csv\n",
      "drwxrwxr-x 4 ghost ghost    4096 8月  10 22:21 P4\n",
      "drwxrwxr-x 2 ghost ghost    4096 8月   3 16:31 patients\n",
      "-rw-rw-r-- 1 ghost ghost   14527 8月   3 16:28 patients-2.csv\n",
      "-rw-rw-r-- 1 ghost ghost   70634 12月 18  2018 patients.csv\n",
      "drwxrwxr-x 2 ghost ghost    4096 8月   7 00:56 payment.json\n",
      "drwxrwxr-x 2 ghost ghost    4096 8月   7 00:05 s3-out-songs.csv\n",
      "drwxrwxr-x 3 ghost ghost    4096 1月  28  2019 song_data\n",
      "-rw-rw-r-- 1 ghost ghost 4458247 11月  1  2018 sparkify_log_small.json\n",
      "-rw-rw-r-- 1 ghost ghost    4792 8月   3 11:52 Test_ReadMe.md\n",
      "total 8\n",
      "drwxrwxr-x 2 ghost ghost 4096 8月  10 22:21 log-data\n",
      "drwxrwxr-x 3 ghost ghost 4096 8月  10 22:21 song_data\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../workdata/\n",
    "!ls -l ../workdata/P4\n",
    "# !ls /home/ghost/workdata/Out-P4/songs_table.parquet/year=1969/artist_id=ARMJAGH1187FB546F3\n",
    "#!ls /home/ghost/workdata/Out-P4/artists.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"dl.cfg\")\n",
    "\n",
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config.get('AWS','AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AKIARJNMFH6UAXJTAOCA' 'TO8A7WFROCfAdb7r3Jibn0ntM7tVh+hjkwjQ8XN9'\n"
     ]
    }
   ],
   "source": [
    "print(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date,lit\n",
    "df.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读 parquet\n",
    "df_parquent = spark.read.format('parquet')\\\n",
    "    .load('/home/ghost/workdata/Out-P4/songplays.parquet')\n",
    "\n",
    "# # 读 parquet，仅某一分区\n",
    "# df_parquent_male = spark.read.format('parquet')\\\n",
    "#     .load('/home/ghost/workdata/out-patient.parquet/assigned_sex=male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+-----------+----+-----+\n",
      "|          start_time|user_id|level|           song_id|         artist_id|session_id|            location|          user_agent|songplay_id|year|month|\n",
      "+--------------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+-----------+----+-----+\n",
      "|2018-11-22 05:56:...|     15| paid|SOZCTXZ12AB0182364|AR5KOSW1187FB35FF4|       818|Chicago-Napervill...|\"Mozilla/5.0 (X11...|          0|2018|   11|\n",
      "+--------------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+-----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquent.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
