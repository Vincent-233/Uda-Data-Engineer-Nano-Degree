{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析SAS字典文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "sas_desc_file = r'/home/ghost/workspace/uda-dend-capstone/data-input/I94_SAS_Labels_Descriptions.SAS'\n",
    "csv_out_path = r'/home/ghost/workspace/uda-dend-capstone/data-upload/csv'\n",
    "\n",
    "def match_to_csv(match_var,csv_file_name):\n",
    "    lines = []\n",
    "    reg_exp = r'\\s*=\\s*'\n",
    "    for line in match_var.split('\\n'):\n",
    "        if line.strip() and re.search(reg_exp,line):    # if not empty line\n",
    "            line_t = re.sub(reg_exp,'|',line.strip().replace(\"'\",''))    # replace xxx = 'yyy' to xxx|yyy\n",
    "            lines.append(line_t + '\\n')\n",
    "    with open(csv_file_name,'w') as fw:\n",
    "        fw.writelines(lines)\n",
    "    \n",
    "if not os.path.exists(csv_out_path):\n",
    "        os.mkdir(csv_out_path)\n",
    "        \n",
    "# sas desc file is small,so read all into one variable \n",
    "with open(sas_desc_file,'r') as file: \n",
    "    sas_desc = file.read()\n",
    "\n",
    "# reg_exp = r'\\*/\\s*value\\s+(\\S+)(.*?);'\n",
    "# matchs = re.findall(reg_exp,sas_desc,re.S)\n",
    "\n",
    "reg_exp = r'/\\* I94VISA(.*?)\\*/'\n",
    "matches = re.findall(reg_exp,sas_desc,re.S)\n",
    "match_to_csv(matches[0],os.path.join(csv_out_path,'i94visa.csv'))\n",
    "\n",
    "\n",
    "# for m in matchs:\n",
    "#     file_name = os.path.join(csv_out_path,m[0].replace('$','') + '.csv')   # removing leading $\n",
    "#     match_to_csv(m[1],file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' - Visa codes collapsed into three categories:\\n   1 = Business\\n   2 = Pleasure\\n   3 = Student\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取SAS文件，转换为parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "\n",
    "columns = [\n",
    "    'cicid', 'i94yr', 'i94mon', \n",
    "    'i94cit', 'i94res', 'i94port',\n",
    "    'arrdate', 'i94mode', 'i94addr',\n",
    "    'depdate', 'i94bir', 'i94visa',\n",
    "    'count', 'dtadfile', 'visapost',\n",
    "    'occup', 'entdepa', 'entdepd',\n",
    "    'entdepu', 'matflag', 'biryear',\n",
    "    'dtaddto', 'gender', 'insnum',\n",
    "    'airline', 'admnum', 'fltno',\n",
    "    'visatype'\n",
    "]\n",
    "\n",
    "schema = \"\"\"\n",
    "    cicid\tdouble,\n",
    "    i94yr\tdouble,\n",
    "    i94mon\tdouble,\n",
    "    i94cit\tdouble,\n",
    "    i94res\tdouble,\n",
    "    i94port\tvarchar(200),\n",
    "    arrdate\tdouble,\n",
    "    i94mode\tdouble,\n",
    "    i94addr\tvarchar(200),\n",
    "    depdate\tdouble,\n",
    "    i94bir\tdouble,\n",
    "    i94visa\tdouble,\n",
    "    count\tdouble,\n",
    "    dtadfile\tvarchar(200),\n",
    "    visapost\tvarchar(200),\n",
    "    occup\tvarchar(200),\n",
    "    entdepa\tvarchar(200),\n",
    "    entdepd\tvarchar(200),\n",
    "    entdepu\tvarchar(200),\n",
    "    matflag\tvarchar(200),\n",
    "    biryear\tdouble,\n",
    "    dtaddto\tvarchar(200),\n",
    "    gender\tvarchar(200),\n",
    "    insnum\tvarchar(200),\n",
    "    airline\tvarchar(200),\n",
    "    admnum\tdouble,\n",
    "    fltno\tvarchar(200),\n",
    "    visatype\tvarchar(200)\n",
    "\"\"\"\n",
    "# read all sas7bdat file under data-input folder\n",
    "df_spark = spark.createDataFrame([],schema) # create a empty dataframe\n",
    "for filename in glob.glob('/home/ghost/workspace/uda-dend-capstone/data-input/*.sas7bdat'):\n",
    "    df_sas = spark.read\\\n",
    "            .format('com.github.saurfang.sas.spark')\\\n",
    "            .schema(schema)\\\n",
    "            .load(filename)\n",
    "    df_spark = df_spark.union(df_sas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.7 ms, sys: 12.1 ms, total: 71.8 ms\n",
      "Wall time: 8min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time # about 8 mins\n",
    "# write to parquet\n",
    "df_spark.write.mode('overwrite').parquet(\"/home/ghost/workspace/uda-de-capstone/data-upload/sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上传到S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# 200M parquet files,about 10 mins\n",
    "import subprocess as sub\n",
    "\n",
    "# params\n",
    "upload_file = 'sas_data.parquet'\n",
    "\n",
    "s3_copy_cmd = [\n",
    "    'aws','s3','cp',\n",
    "    f'/home/ghost/workspace/uda-dend-capstone/data-upload/{upload_file}',\n",
    "    f's3://bucket-vincent-archive/{upload_file}',\n",
    "    '--recursive','--exclude','*.crc'\n",
    "]\n",
    "result = sub.run(s3_copy_cmd,stdout=sub.PIPE,stderr=sub.PIPE)\n",
    "if result.returncode:\n",
    "    print(result.stderr)\n",
    "    print('upload to s3 faild')\n",
    "else:\n",
    "    print('successfully upload files to s3.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法二（建议采用此方法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_id = 'xxx'\n",
    "secret_key = 'xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading i94model.csv\n",
      "i94model.csv uploaded.\n",
      "uploading us_cities_demographics.csv\n",
      "us_cities_demographics.csv uploaded.\n",
      "uploading i94prtl.csv\n",
      "i94prtl.csv uploaded.\n",
      "uploading i94addrl.csv\n",
      "i94addrl.csv uploaded.\n",
      "uploading airport_codes.csv\n",
      "airport_codes.csv uploaded.\n",
      "uploading i94visa.csv\n",
      "i94visa.csv uploaded.\n",
      "uploading i94cntyl.csv\n",
      "i94cntyl.csv uploaded.\n",
      "all files uploaded.\n",
      "CPU times: user 512 ms, sys: 140 ms, total: 651 ms\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "def upload_files(bucket,path,exclude):\n",
    "    \"\"\"upload files in path with exclude file pattern\"\"\"\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id = access_id,\n",
    "        aws_secret_access_key = secret_key,\n",
    "        region_name = 'us-west-2'\n",
    "    )\n",
    "    s3 = session.resource('s3')\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            if not fnmatch.fnmatch(full_path,exclude):\n",
    "                with open(full_path, 'rb') as data:\n",
    "                    relative_path = full_path[len(path.rstrip('/')) + 1:]\n",
    "                    print(f'uploading {relative_path}')\n",
    "                    bucket.put_object(Key = relative_path, Body = data)\n",
    "                    print(f'{relative_path} uploaded.')\n",
    "    print('all files uploaded.')\n",
    "\n",
    "# test\n",
    "# upload_files('bucket-vincent-archive','/home/ghost/workspace/uda-dend-capstone/data-upload','*.crc')\n",
    "upload_files('bucket-vincent-archive','/home/ghost/workspace/uda-dend-capstone/data-upload/csv','*.crc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读 Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建或获取会话\n",
    "spark = SparkSession.builder.appName('Python Spark SQL example').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .option('sep', ';')\\\n",
    "    .load('/home/ghost/workspace/uda-dend-capstone/data-upload/csv/us_cities_demographics.csv')\n",
    "\n",
    "# rename column,or space with rasise a error\n",
    "for col_name in df.columns:\n",
    "    df = df.withColumnRenamed(col_name,col_name.replace(' ','-'))\n",
    "\n",
    "# write to parquet    \n",
    "df.write.mode('overwrite').partitionBy('Race').parquet('/home/ghost/workspace/uda-dend-capstone/data-upload/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['City',\n",
       " 'State',\n",
       " 'Median-Age',\n",
       " 'Male-Population',\n",
       " 'Female-Population',\n",
       " 'Total-Population',\n",
       " 'Number-of-Veterans',\n",
       " 'Foreign-born',\n",
       " 'Average-Household-Size',\n",
       " 'State-Code',\n",
       " 'Race',\n",
       " 'Count']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.write.mode('overwrite').partitionBy('Race').parquet('/home/ghost/workspace/uda-dend-capstone/data-upload/test.parquet')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读 parquet\n",
    "df_parquent = spark.read.format('parquet')\\\n",
    "    .load('/home/ghost/workspace/uda-dend-capstone/data-upload/test.parquet/Race=Asian/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['City',\n",
       " 'State',\n",
       " 'Median-Age',\n",
       " 'Male-Population',\n",
       " 'Female-Population',\n",
       " 'Total-Population',\n",
       " 'Number-of-Veterans',\n",
       " 'Foreign-born',\n",
       " 'Average-Household-Size',\n",
       " 'State-Code',\n",
       " 'Count']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以发现仅仅只读单个的parquet，结果中是没有被分区的列的\n",
    "df_parquent.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 临时测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "x = 324\n",
    "if x:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import shutil\n",
    "import distutils.dir_util as dir_util\n",
    "from datetime import datetime, timedelta\n",
    "exclude = '*.crc'\n",
    "full_path = 'sas_data/.part-00006-b951a2bb-402f-476d-97e3-84740450bd69-c000.snappy.parquet.crc'\n",
    "full_path_2 = '546.crc'\n",
    "print(fnmatch.fnmatch(full_path,exclude))\n",
    "print(fnmatch.fnmatch(full_path_2,exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ghost'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d53a6adad308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a = {},b = {{}},c = {}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "x = 'a = {},b = {{}},c = {}'\n",
    "x.format(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 4, 'c': 5}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {'a':1,'b':3}\n",
    "x.update({'b':4,'c':5})\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 10, 19, 15, 45, 7, 357878)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datedatetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear input folder\n",
    "def clear_folder(data_input):\n",
    "    for name in os.listdir(data_input):\n",
    "        file = os.path.join(data_input,name)\n",
    "        try:\n",
    "            dir_util.remove_tree(file)\n",
    "        except OSError:\n",
    "            os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_archive = '/home/ghost/workspace/uda-dend-capstone/data-input-archive'\n",
    "data_input = '/home/ghost/workspace/uda-dend-capstone/data-input'\n",
    "fold_name = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "# dir_util.copy_tree(data_input,os.path.join(data_input_archive,fold_name)) # copy file to archive folder\n",
    "# clear_folder(data_input) # clear input folder\n",
    "# clear_folder(upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ghost/workspace/uda-dend-capstone/data-input-archive/20191019/data-input.zip',\n",
       " '/home/ghost/workspace/uda-dend-capstone/data-input-archive/20191019/i94_feb16_sub.sas7bdat',\n",
       " '/home/ghost/workspace/uda-dend-capstone/data-input-archive/20191019/us-cities-demographics.csv',\n",
       " '/home/ghost/workspace/uda-dend-capstone/data-input-archive/20191019/immigration_data_sample.csv',\n",
       " '/home/ghost/workspace/uda-dend-capstone/data-input-archive/20191019/.ipynb_checkpoints/immigration_data_sample-checkpoint.csv',\n",
       " '/home/ghost/workspace/uda-dend-capstone/data-input-archive/20191019/.ipynb_checkpoints/airport-codes_csv-checkpoint.csv',\n",
       " '/home/ghost/workspace/uda-dend-capstone/data-input-archive/20191019/.ipynb_checkpoints/I94_SAS_Labels_Descriptions-checkpoint.SAS',\n",
       " '/home/ghost/workspace/uda-dend-capstone/data-input-archive/20191019/I94_SAS_Labels_Descriptions.SAS',\n",
       " '/home/ghost/workspace/uda-dend-capstone/data-input-archive/20191019/airport-codes_csv.csv']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shutil.copytree(data_input,os.path.join(data_input_archive,fold_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_util.remove_tree(data_input)\n",
    "# shutil.rmtree(data_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# from shutil import copy\n",
    "# for name in glob.glob('/home/ghost/workspace/uda-dend-capstone/data-input/**/**.csv',recursive=True):\n",
    "#     copy(name,os.path.join(data_input,'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ghost/workspace/uda-dend-capstone/data-input/test/airport_codes.csv'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy('/home/ghost/workspace/uda-dend-capstone/data-input/airport_codes.csv','/home/ghost/workspace/uda-dend-capstone/data-input/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ghost/workspace/uda-dend-capstone/data-input/airport_codes.csv'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.path.basename('/home/ghost/workspace/uda-dend-capstone/data-input/airport_codes.csv')\n",
    "os.path.join('/home/ghost/workspace/uda-dend-capstone/data-input/','airport_codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "import pandas as pd  \n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ghost/workspace/uda-dend-capstone/data-input/iso_3166_2.json') as f:\n",
    "    country_region_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>divisions.CN-11</th>\n",
       "      <th>divisions.CN-50</th>\n",
       "      <th>divisions.CN-31</th>\n",
       "      <th>divisions.CN-12</th>\n",
       "      <th>divisions.CN-34</th>\n",
       "      <th>divisions.CN-35</th>\n",
       "      <th>divisions.CN-62</th>\n",
       "      <th>divisions.CN-44</th>\n",
       "      <th>divisions.CN-52</th>\n",
       "      <th>...</th>\n",
       "      <th>divisions.CN-71</th>\n",
       "      <th>divisions.CN-53</th>\n",
       "      <th>divisions.CN-33</th>\n",
       "      <th>divisions.CN-45</th>\n",
       "      <th>divisions.CN-15</th>\n",
       "      <th>divisions.CN-64</th>\n",
       "      <th>divisions.CN-65</th>\n",
       "      <th>divisions.CN-54</th>\n",
       "      <th>divisions.CN-91</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Chongqing</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>Tianjin</td>\n",
       "      <td>Anhui</td>\n",
       "      <td>Fujian</td>\n",
       "      <td>Gansu</td>\n",
       "      <td>Guangdong</td>\n",
       "      <td>Guizhou</td>\n",
       "      <td>...</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>Yunnan</td>\n",
       "      <td>Zhejiang</td>\n",
       "      <td>Guangxi</td>\n",
       "      <td>Nei Monggol</td>\n",
       "      <td>Ningxia</td>\n",
       "      <td>Xinjiang</td>\n",
       "      <td>Xizang</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name divisions.CN-11 divisions.CN-50 divisions.CN-31 divisions.CN-12  \\\n",
       "0  China         Beijing       Chongqing        Shanghai         Tianjin   \n",
       "\n",
       "  divisions.CN-34 divisions.CN-35 divisions.CN-62 divisions.CN-44  \\\n",
       "0           Anhui          Fujian           Gansu       Guangdong   \n",
       "\n",
       "  divisions.CN-52  ... divisions.CN-71 divisions.CN-53 divisions.CN-33  \\\n",
       "0         Guizhou  ...          Taiwan          Yunnan        Zhejiang   \n",
       "\n",
       "  divisions.CN-45 divisions.CN-15 divisions.CN-64 divisions.CN-65  \\\n",
       "0         Guangxi     Nei Monggol         Ningxia        Xinjiang   \n",
       "\n",
       "  divisions.CN-54 divisions.CN-91 country_code  \n",
       "0          Xizang       Hong Kong           CN  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iso_country_region = country_region_json\n",
    "df_iso_country_region = json_normalize(iso_country_region['CN'])\n",
    "df_iso_country_region['country_code'] = 'CN'\n",
    "df_iso_country_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivot_cols = [col for col in list(df_iso_country_region) if col not in ('name','country_code')]\n",
    "df_iso_country_region = pd.melt(df_iso_country_region,id_vars=['country_code','name'],value_vars=unpivot_cols).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iso_country_region['variable'] = df_iso_country_region['variable'].str.replace('divisions.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>name</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>CN-11</td>\n",
       "      <td>Beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>CN-50</td>\n",
       "      <td>Chongqing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>CN-31</td>\n",
       "      <td>Shanghai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>CN-12</td>\n",
       "      <td>Tianjin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>CN-34</td>\n",
       "      <td>Anhui</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_code   name variable      value\n",
       "0           CN  China    CN-11    Beijing\n",
       "1           CN  China    CN-50  Chongqing\n",
       "2           CN  China    CN-31   Shanghai\n",
       "3           CN  China    CN-12    Tianjin\n",
       "4           CN  China    CN-34      Anhui"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iso_country_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以方法就是一个个解析，然后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_region_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析奇怪的嵌套JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "import pandas as pd  \n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "with open('/home/ghost/workspace/uda-dend-capstone/data-input/iso_3166_2.json') as f:\n",
    "    country_region_json = json.load(f)\n",
    "\n",
    "cols = ['country_code','country_name','region_code','region_name']\n",
    "df_iso_country_region = pd.DataFrame(columns = cols)\n",
    "\n",
    "for country_code in country_region_json:\n",
    "    df_country_region = json_normalize(iso_country_region[country_code])\n",
    "    df_country_region['country_code'] = country_code\n",
    "    df_country_region.rename(columns = {'name':'country_name'},inplace = True)\n",
    "    unpivot_cols = [col for col in list(df_country_region) if col not in ('country_name','country_code')]\n",
    "    df_country_region = pd.melt(df_country_region,id_vars=['country_code','country_name'],value_vars = unpivot_cols\n",
    "                               ,var_name = 'region_code',value_name = 'region_name')\n",
    "    df_country_region['region_code'] = df_country_region['region_code'].str.replace('divisions.','')\n",
    "    df_iso_country_region = df_iso_country_region.append(df_country_region,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iso_country_region.to_json('/home/ghost/workspace/uda-dend-capstone/data-upload/iso_country_region.json',orient = 'table',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iso_country_region.to_csv('/home/ghost/workspace/uda-dend-capstone/data-upload/iso_country_region.csv',sep='|',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
